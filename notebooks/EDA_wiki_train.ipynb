{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d654587e-a007-4997-9037-d63521c4e351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydenchiu/miniconda3/envs/gpt-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(os.path.expanduser(\"~/git/gpt_from_scratch/\"))\n",
    "from src.data.load_data import *\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac5684c-cde6-4313-a769-1afded4aeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WikiText-2-raw-v1 dataset\n",
    "dataset = load_dataset(\"Bingsu/openwebtext_20p\")\n",
    "train_data = dataset['train']\n",
    "# valid_data = dataset['validation']\n",
    "# test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9329d691-716c-40b0-8f65-ba988f4d9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'If you live abroad and are requesting an ITIN for a foreign child who has been adopted or legally placed in your home pending adoption, remember to include a copy of the legal documents evidencing your relationship to the child.'}\n"
     ]
    }
   ],
   "source": [
    "# Sample a few data points to see the content\n",
    "print(train_data[0])\n",
    "# print(valid_data[0])\n",
    "# print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18d970f-8392-489d-b84a-4ec562b2d294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training documents: 33167823\n",
      "Average words per document: 39.604504281152245\n",
      "Median words per document: 28.0\n",
      "Max words in a document: 20975\n",
      "Min words in a document: 1\n"
     ]
    }
   ],
   "source": [
    "# Number of documents in each split\n",
    "print(f\"Number of training documents: {len(train_data)}\")\n",
    "# print(f\"Number of validation documents: {len(valid_data)}\")\n",
    "# print(f\"Number of test documents: {len(test_data)}\")\n",
    "\n",
    "# Word counts per document\n",
    "word_counts = [len(doc['text'].split()) for doc in train_data]\n",
    "print(f\"Average words per document: {np.mean(word_counts)}\")\n",
    "print(f\"Median words per document: {np.median(word_counts)}\")\n",
    "print(f\"Max words in a document: {np.max(word_counts)}\")\n",
    "print(f\"Min words in a document: {np.min(word_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8871f2f-c16a-4d87-a9dc-884a5d9d3332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1190 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per document: 52.61460346673944\n",
      "Median tokens per document: 36.0\n",
      "Max tokens in a document: 75147\n",
      "Min tokens in a document: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Tokenize the dataset\n",
    "token_lengths = [len(tokenizer.encode(doc['text'])) for doc in train_data]\n",
    "\n",
    "print(f\"Average tokens per document: {np.mean(token_lengths)}\")\n",
    "print(f\"Median tokens per document: {np.median(token_lengths)}\")\n",
    "print(f\"Max tokens in a document: {np.max(token_lengths)}\")\n",
    "print(f\"Min tokens in a document: {np.min(token_lengths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a8eee-80b6-48ed-8c4f-c02c3006e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histogram of word counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(word_counts, bins=50, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Document Lengths (in Words)')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(token_lengths, bins=50, color='green', edgecolor='black')\n",
    "plt.title('Distribution of Document Lengths (in Tokens)')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06e496-fa3c-4fef-ba75-9372066e2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Tokenize words for frequency analysis\n",
    "all_words = [word for doc in train_data for word in doc['text'].split()]\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Top 20 most common words\n",
    "print(\"Top 20 most common words:\")\n",
    "print(word_freq.most_common(20),\"\\n\")\n",
    "\n",
    "# Bigram analysis\n",
    "bigrams = list(ngrams(all_words, 2))\n",
    "bigram_freq = Counter(bigrams)\n",
    "print(\"Top 20 most common bigrams:\")\n",
    "print(bigram_freq.most_common(20),\"\\n\")\n",
    "\n",
    "# Trigram analysis\n",
    "trigrams = list(ngrams(all_words, 3))\n",
    "trigram_freq = Counter(trigrams)\n",
    "print(\"Top 20 most common trigrams:\")\n",
    "print(trigram_freq.most_common(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc78aa0-29ec-4ee9-b209-7ef6f8893f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size\n",
    "vocab_size = len(word_freq)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Words that appear only once\n",
    "rare_words = [word for word, count in word_freq.items() if count == 1]\n",
    "print(f\"Number of rare words (appearing once): {len(rare_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c48fe-f45a-49dd-bd44-9b85ccf601b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "# Tokenize sentences\n",
    "sent_lengths = []\n",
    "for doc in train_data:\n",
    "    sentences = nltk.sent_tokenize(doc['text'])\n",
    "    sent_lengths.extend([len(sentence.split()) for sentence in sentences])\n",
    "\n",
    "# Sentence length statistics\n",
    "print(f\"Average sentence length: {np.mean(sent_lengths)} words\")\n",
    "print(f\"Median sentence length: {np.median(sent_lengths)} words\")\n",
    "print(f\"Max sentence length: {np.max(sent_lengths)} words\")\n",
    "print(f\"Min sentence length: {np.min(sent_lengths)} words\")\n",
    "\n",
    "# Histogram of sentence lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sent_lengths, bins=50, color='purple', edgecolor='black')\n",
    "plt.title('Distribution of Sentence Lengths (in Words)')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1b9a5-b5e0-4475-8e11-499f9e01bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Count special characters\n",
    "special_chars = re.findall(r'[^\\w\\s]', ' '.join(all_words))\n",
    "special_char_freq = Counter(special_chars)\n",
    "\n",
    "print(\"Most common special characters:\")\n",
    "print(special_char_freq.most_common(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d1cbb-3b83-47fa-86cc-d96ed54d1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate a word cloud for frequent words\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='black').generate(' '.join(all_words))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of the Training Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5c48e-a85d-46ed-aa4b-ffba5a6077fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df75df-06c3-484a-8839-50b946d183c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
