{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192f0a09-7966-49f9-8633-266b550d2b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydenchiu/miniconda3/envs/gpt-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset  # huggingface data loading function\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3158724-f47f-4dae-803c-70fd5a63098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a60b3c-515e-4efb-9201-fa9db021a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Tokenize each text sample and return input IDs and attention mask\"\"\"\n",
    "        # return self.data[index]\n",
    "        # Tokenize each text sample and return input IDs and attention mask\n",
    "        tokenized = self.tokenizer(\n",
    "            self.data[index],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze()\n",
    "        }\n",
    "    \n",
    "def get_wikitext_data(tokenizer_name=\"gpt2\", max_length=512):\n",
    "    \"\"\"Load raw text data from wikitext dataset\"\"\"\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "    train_texts = dataset[\"train\"][\"text\"]\n",
    "    valid_texts = dataset[\"validation\"][\"text\"]\n",
    "    test_texts = dataset[\"test\"][\"text\"]\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, clean_up_tokenization_spaces=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Ensure compatibility for padding\n",
    "\n",
    "    # Wrap text data in the TextDataset with tokenization\n",
    "    train_data = TextDataset(train_texts, tokenizer, max_length=max_length)\n",
    "    valid_data = TextDataset(valid_texts, tokenizer, max_length=max_length)\n",
    "    test_data = TextDataset(test_texts, tokenizer, max_length=max_length)\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "def get_char_mappings(tokenizer_name=\"gpt2\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, clean_up_tokenization_spaces=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Ensure compatibility for padding\n",
    "\n",
    "    # `char_to_int` maps tokens to their respective ids\n",
    "    char_to_int = tokenizer.get_vocab()\n",
    "    # `int_to_char` maps ids back to their tokens\n",
    "    int_to_char = {v: k for k, v in char_to_int.items()}\n",
    "    \n",
    "    return char_to_int, int_to_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b90dc033-bfd4-452b-a135-63158c18e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = get_wikitext_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783903d3-73d4-49da-aeb7-0800d51ea552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch keys: dict_keys(['input_ids', 'attention_mask'])\n",
      "Sample input_ids batch shape: torch.Size([2, 512])\n",
      "Sample attention_mask batch shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Sample data loader\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Sample batch keys:\", sample_batch.keys())\n",
    "print(\"Sample input_ids batch shape:\", sample_batch[\"input_ids\"].shape)\n",
    "print(\"Sample attention_mask batch shape:\", sample_batch[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "460f1563-f73a-4a13-a02b-ec4ccc379f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "Special tokens: ['<|endoftext|>']\n",
      "Special tokens mapping: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", clean_up_tokenization_spaces=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure compatibility for padding\n",
    "\n",
    "# Assuming `tokenizer` is loaded as in your code\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", tokenizer.all_special_tokens)\n",
    "print(\"Special tokens mapping:\", tokenizer.special_tokens_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3784c996-5741-46de-9377-5c67eac40923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample vocabulary tokens:\n",
      "Token ID 0: !\n",
      "Token ID 1: \"\n",
      "Token ID 2: #\n",
      "Token ID 3: $\n",
      "Token ID 4: %\n",
      "Token ID 5: &\n",
      "Token ID 6: '\n",
      "Token ID 7: (\n",
      "Token ID 8: )\n",
      "Token ID 9: *\n"
     ]
    }
   ],
   "source": [
    "# Get a list of most common tokens in the vocabulary (if available)\n",
    "print(\"Sample vocabulary tokens:\")\n",
    "for i in range(10):\n",
    "    token = tokenizer.convert_ids_to_tokens(i)\n",
    "    print(f\"Token ID {i}: {token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd9b6c94-e669-4119-9cdc-a7b117a85f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character(s): '@'\n",
      "  Tokens: ['@']\n",
      "  Token IDs: [31]\n",
      "Character(s): '.'\n",
      "  Tokens: ['.']\n",
      "  Token IDs: [13]\n",
      "Character(s): ' '\n",
      "  Tokens: ['Ġ']\n",
      "  Token IDs: [220]\n",
      "Character(s): 'Hello, this is a test.'\n",
      "  Tokens: ['Hello', ',', 'Ġthis', 'Ġis', 'Ġa', 'Ġtest', '.']\n",
      "  Token IDs: [15496, 11, 428, 318, 257, 1332, 13]\n"
     ]
    }
   ],
   "source": [
    "# Testing characters like '@', '.', spaces, etc.\n",
    "test_characters = [\"@\", \".\", \" \", \"Hello, this is a test.\"]\n",
    "for char in test_characters:\n",
    "    tokens = tokenizer.tokenize(char)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"Character(s): '{char}'\")\n",
    "    print(\"  Tokens:\", tokens)\n",
    "    print(\"  Token IDs:\", token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64a22e49-7383-4ba4-bddb-128add69baae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello, this is a test.\n",
      "Tokens: ['Hello', ',', 'Ġthis', 'Ġis', 'Ġa', 'Ġtest', '.']\n",
      "Token IDs: [15496, 11, 428, 318, 257, 1332, 13]\n",
      "Reconstructed Text: Hello, this is a test.\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "sample_text = \"Hello, this is a test.\"\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "reconstructed_text = tokenizer.decode(token_ids)\n",
    "\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Reconstructed Text:\", reconstructed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "876c06fd-b853-47d1-9caa-dcc7f957b244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for unusual characters: ['@@@@', '@', 'Ġ...']\n"
     ]
    }
   ],
   "source": [
    "# Testing unusual characters\n",
    "unusual_text = \"@@@@@ ...\"\n",
    "tokens = tokenizer.tokenize(unusual_text)\n",
    "print(\"Tokens for unusual characters:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7916a42-44aa-4ac9-91e1-6e648c808f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "valid_texts = dataset[\"validation\"][\"text\"]\n",
    "test_texts = dataset[\"test\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cdaae34-f55c-4f86-b117-336ad54d7b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_char_mappings(tokenizer_name=\"gpt2\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c18c99c-9af2-4d77-8655-6730455ceeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_char_mappings(tokenizer_name=\"gpt2\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a79e1207-83c6-4117-94c6-72bcf460870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(train_texts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d021175-1566-479c-ad25-111d5b64ce47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19372\n",
      " When xenon atoms are at their ground energy state , they repel each other and will not form a bond . When xenon atoms becomes energized , however , they can form an excimer ( excited dimer ) until the electrons return to the ground state . This entity is formed because the xenon atom tends to fill its outermost electronic shell , and can briefly do this by adding an electron from a neighboring xenon atom . The typical lifetime of a xenon excimer is 1 – 5 ns , and the decay releases photons with wavelengths of about 150 and 173 nm . Xenon can also form excimers with other elements , such as the halogens bromine , chlorine and fluorine . \n",
      "\n",
      "19385\n",
      " The individual cells in a plasma display use a mixture of xenon and neon that is converted into a plasma using electrodes . The interaction of this plasma with the electrodes generates ultraviolet photons , which then excite the phosphor coating on the front of the display . \n",
      "\n",
      "19413\n",
      " Gamma emission from the radioisotope 133Xe of xenon can be used to image the heart , lungs , and brain , for example , by means of single photon emission computed tomography . 133Xe has also been used to measure blood flow . \n",
      "\n",
      "36226\n",
      " The production of energy at the core is the reason stars shine so brightly : every time two or more atomic nuclei fuse together to form a single atomic nucleus of a new heavier element , gamma ray photons are released from the nuclear fusion product . This energy is converted to other forms of electromagnetic energy of lower frequency , such as visible light , by the time it reaches the star 's outer layers . \n",
      "\n",
      "36265\n",
      " As atomic nuclei are fused in the core , they emit energy in the form of gamma rays . These photons interact with the surrounding plasma , adding to the thermal energy at the core . Stars on the main sequence convert hydrogen into helium , creating a slowly but steadily increasing proportion of helium in the core . Eventually the helium content becomes predominant , and energy production ceases at the core . Instead , for stars of more than 0 @.@ 4 M ☉ , fusion occurs in a slowly expanding shell around the degenerate helium core . \n",
      "\n",
      "36269\n",
      " The photosphere is that portion of a star that is visible to an observer . This is the layer at which the plasma of the star becomes transparent to photons of light . From here , the energy generated at the core becomes free to propagate into space . It is within the photosphere that sun spots , regions of lower than average temperature , appear . \n",
      "\n",
      "36284\n",
      " where e + is a positron , γ is a gamma ray photon , νe is a neutrino , and H and He are isotopes of hydrogen and helium , respectively . The energy released by this reaction is in millions of electron volts , which is actually only a tiny amount of energy . However enormous numbers of these reactions occur constantly , producing all the energy necessary to sustain the star 's radiation output . In comparison , the combustion of two hydrogen gas molecules with one oxygen gas molecule releases only 5 @.@ 7 eV . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n, text in enumerate(train_texts):\n",
    "    if \"photon\" in text:\n",
    "        print(n)\n",
    "        print(text)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "500dc9ad-bc78-411f-8919-6a6762a2b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(train_texts[87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92d639eb-97a1-4c8a-9e5f-3e6d20380e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc732c10-0d35-44cb-a0d8-de2ca8ef1e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4210]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"�\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba9a5981-a029-41ea-b19f-24282e2beff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(4210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a1cdc39-143d-4d6c-9b6d-4c05da7b0ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                               | 0/18359 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[50256, 50256, 50256,  ..., 50256, 50256, 50256],\n",
      "        [ 5053,   907,  2540,  ..., 50256, 50256, 50256]], device='mps:0')\n",
      "input ids shape torch.Size([2, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                               | 0/18359 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded input_ids <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "target_ids: tensor([[50256, 50256, 50256,  ..., 50256, 50256, 50256],\n",
      "        [  907,  2540, 16381,  ..., 50256, 50256, 50256]], device='mps:0')\n",
      "decoded target_ids <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_texts\n",
    "\n",
    "for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "    # batch = batch.to(device)\n",
    "    batch = batch[\"input_ids\"].to(device)\n",
    "    \n",
    "    # Shift input for language modeling\n",
    "    input_ids = batch[:, :-1]\n",
    "    print(\"input_ids:\", input_ids)\n",
    "    print(\"input ids shape\", input_ids.shape)\n",
    "    print(\"decoded input_ids\", tokenizer.decode([in_id.item() for in_id in input_ids[0]], clean_up_tokenization_spaces = True))\n",
    "    target_ids = batch[:, 1:]\n",
    "    print(\"target_ids:\", target_ids)\n",
    "    print(\"decoded target_ids\", tokenizer.decode([tar_id.item() for tar_id in target_ids[0]]))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b796dc7a-4f5e-4097-acf8-fa94e08ed7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openwebtext_data(tokenizer_name=\"gpt2\", max_length=512):\n",
    "    \"\"\"Load raw text data from open_webtext dataset\"\"\"\n",
    "    dataset = load_dataset(\"Bingsu/openwebtext_20p\")\n",
    "    train_texts = dataset[\"train\"][\"text\"]\n",
    "    # valid_texts = dataset[\"validation\"][\"text\"]\n",
    "    # test_texts = dataset[\"test\"][\"text\"]\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, clean_up_tokenization_spaces=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Ensure compatibility for padding\n",
    "\n",
    "    # Wrap text data in the TextDataset with tokenization\n",
    "    train_data = TextDataset(train_texts, tokenizer, max_length=max_length)\n",
    "    # valid_data = TextDataset(valid_texts, tokenizer, max_length=max_length)\n",
    "    # test_data = TextDataset(test_texts, tokenizer, max_length=max_length)\n",
    "\n",
    "    return train_data#, valid_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0af39f21-c93a-4f06-89ce-b9e78474dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_openwebtext_data(tokenizer_name=\"gpt2\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1083b4b3-98ad-4011-9768-563ffcad2dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch keys: dict_keys(['input_ids', 'attention_mask'])\n",
      "Sample input_ids batch shape: torch.Size([2, 512])\n",
      "Sample attention_mask batch shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Sample data loader\n",
    "train_loader = DataLoader(train, batch_size=2, shuffle=True)\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Sample batch keys:\", sample_batch.keys())\n",
    "print(\"Sample input_ids batch shape:\", sample_batch[\"input_ids\"].shape)\n",
    "print(\"Sample attention_mask batch shape:\", sample_batch[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ff30d7d-ef30-43f1-8f5e-35106064b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Bingsu/openwebtext_20p\")\n",
    "train_texts = dataset[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9411f08-d787-4012-be59-793fbe592f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24018\n",
      "In our homes, we spend an incredible amount of energy heating our water and that’s sort of ironic given that we have so many photons and so much sunshine that will heat the water for us for free. And so an easy option for those homes that have rooftops that are not shaded is to get solar water heaters installed. My household had its solar water heater installed in the ‘80s, and it’s still working great and it saves us a ton of money and it helps reduce our energy consumption overall. It works great. So that’s an easy solution, an easy option for a lot of people.\n"
     ]
    }
   ],
   "source": [
    "for n, text in enumerate(train_texts):\n",
    "    if \"photon\" in text:\n",
    "        print(n)\n",
    "        print(text)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638144f-c5af-4a58-8b31-b78701d3592d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
